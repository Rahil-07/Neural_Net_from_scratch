{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd5054d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training-X set before flatten:  (3500, 64, 64, 3)\n",
      "Shape of Testing-X set before flatten:  (500, 64, 64, 3)\n",
      "Shape of Training-y set:  (1, 3500)\n",
      "Shape of Testing-y set:  (1, 500)\n",
      "Shape of Training-X set after flatten:  (12288, 3500)\n",
      "Shape of Testing-X set after flatten:  (12288, 500)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "from dataset_initialization import train_x_set,train_y_set,test_x_set,test_y_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1007ed",
   "metadata": {},
   "source": [
    "## Initializing parameters\n",
    "\n",
    "parameters 'W' and 'B' initialized for each layer. \n",
    "\n",
    "function 'initialize_params()' take argument as list \"layer_dims\", which contain no.of nuerons in each layer.\n",
    "\n",
    "i.e. layer_dims = [64,16,8,1]\n",
    "\n",
    "layer0(input_layer) has 64 nuerons , layer1 = 16 , layer2 = 8 , layer4(output_layer) = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7612e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    \n",
    "    parameters = {}\n",
    "    \n",
    "    for i in range(1,len(layer_dims)):\n",
    "        \n",
    "        parameters['W'+str(i)] = np.random.randn(layer_dims[i],layer_dims[i-1] ) * math.sqrt(2./layer_dims[i-1])\n",
    "        parameters['B'+str(i)] = np.zeros((layer_dims[i],1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fcdc40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea8108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / ( 1 + np.exp(-x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939d6f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afdec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3ad62b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a663269",
   "metadata": {},
   "source": [
    "## Forward propogation\n",
    "\n",
    "The linear forward module (vectorized over all the examples) computes the following equations:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$$\n",
    "\n",
    "where $A^{[0]} = X(input_layer)$.\n",
    "\n",
    "return cache dictionary which contain 'Z' and 'A' for each layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0670c101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X,parameters):\n",
    "    A = X\n",
    "    cache = {}\n",
    "    L = int(len(parameters)/2)\n",
    "    \n",
    "    for i in range(1,L+1):\n",
    "        A_prev = A\n",
    "        \n",
    "        Z = np.dot(parameters['W'+str(i)],A_prev) + parameters['B'+str(i)]\n",
    "        \n",
    "        if layer_info[i-1][1] == 'relu':\n",
    "            A = np.maximum(0,Z)\n",
    "        elif layer_info[i-1][1] == 'sigmoid':\n",
    "            A = 1 / ( 1 + np.exp(-Z) )\n",
    "        elif layer_info[i-1][1] == 'tanh':\n",
    "            A = np.tanh(Z)\n",
    "        cache['Z'+str(i)] = Z\n",
    "        cache['A'+str(i)] = A\n",
    "\n",
    "    return  A, cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0177baa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f45e47d8",
   "metadata": {},
   "source": [
    "##  Backward Propogation\n",
    "\n",
    "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
    "\n",
    "Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]} dA^{[l-1]})$.\n",
    "\n",
    "<img src=\"images/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n",
    "<caption><center> **Figure 4** </center></caption>\n",
    "\n",
    "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l]})$ are computed using the input $dZ^{[l]}$.Here are the formulas you need:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70126e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(AL, Y, X, caches, parameters):\n",
    "    grads = {}\n",
    "    L = int(len(caches)/2)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) \n",
    "    \n",
    "#     last layer dA\n",
    "    dA = -(Y/AL - (1-Y)/(1-AL))\n",
    "    \n",
    "    for i in range(L,1,-1):\n",
    "        \n",
    "        if layer_info[i-1][1] == 'relu':\n",
    "            dZ = np.array(dA, copy=True)\n",
    "            dZ[caches['Z'+str(i)] <= 0] = 0\n",
    "        elif layer_info[i-1][1] == 'sigmoid':\n",
    "            s = 1 / (1 + np.exp(-caches['Z'+str(i)]))\n",
    "            dZ = dA * s * (1-s)\n",
    "        elif layer_info[i-1][1] == 'tanh':\n",
    "            dZ = np.dot(parameters['W'+str(i+1)].T,dZ) * (1 - np.power(caches['A'+str(i)],2))\n",
    "            \n",
    "        grads['dW'+str(i)] = np.dot(dZ,caches['A'+str(i-1)].T) / m\n",
    "        grads['dB'+str(i)] = np.sum(dZ) / m\n",
    "        \n",
    "        dA = np.dot(parameters['W'+str(i)].T,dZ)\n",
    "    \n",
    "    #  for first layer\n",
    "    if layer_info[0][1] == 'relu':\n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[caches['Z'+str(1)] <= 0] = 0\n",
    "    elif layer_info[0][1] == 'sigmoid':\n",
    "        s = 1 / (1 + np.exp(-caches['Z'+str(1)]))\n",
    "        dZ = dA * s * (1-s)\n",
    "    elif layer_info[0][1] == 'tanh':\n",
    "        dZ = np.dot(parameters['W'+str(2)].T,dZ) * (1 - np.power(caches['A'+str(1)],2))\n",
    "    \n",
    "    grads['dW1'] = np.dot(dZ,X.T) / m\n",
    "    grads['dB1'] = np.sum(dZ) / m\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dadec72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1a77813",
   "metadata": {},
   "source": [
    "### Update Parameters\n",
    "\n",
    "In this section you will update the parameters of the model, using gradient descent: \n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$\n",
    "\n",
    "where $\\alpha$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a0a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(parameters, grads, learning_rate):\n",
    "    L = int(len(parameters)/2)\n",
    "    \n",
    "    for i in range(1,L+1):\n",
    "        parameters['W'+str(i)] -= learning_rate * grads['dW'+str(i)]\n",
    "        parameters['B'+str(i)] -= learning_rate * grads['dB'+str(i)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f701e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2fb0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,parameters):\n",
    "    AL , cache = forward(X,parameters)\n",
    "    predictions = (AL > 0.5) * 1.0\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88229323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46a02330",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "\n",
    "Now you will implement forward and backward propagation. You need to compute the cost, because you want to check if your model is actually learning.\n",
    "\n",
    "Compute the cross-entropy cost $J$, using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))Â $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc7345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = - (np.dot(Y, np.log(AL).T) + np.dot((1 - Y), np.log(1 - AL).T)) / m\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b504965f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52bec112",
   "metadata": {},
   "source": [
    "### intialization of layer info\n",
    "\n",
    "1. **\"Enter number of layer including output  layer :** i.e. **3** (don't include input layer )\" \n",
    "2. **\"Layer 1 -> Enter no. of neurons and activation function : \"** i.e. **16 relu**\n",
    "3. **\"Layer 2 -> Enter no. of neurons and activation function : \"** i.e. **8 relu**\n",
    "4. **\"Layer 3 -> Enter no. of neurons and activation function : \"** i.e. **1 sigmoid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b8f2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_initialization():\n",
    "    \n",
    "    layers = int(input(\"Enter number of layer including output layer : \"))\n",
    "    layer_info = []\n",
    "    for i in range(layers):\n",
    "        neurons , activation = input(\"Layer \"+str(i+1)+\" -> Enter no. of neurons and activation function : \").split(' ')\n",
    "        layer_info.append([int(neurons),activation])\n",
    "    layer_dims = [ i[0] for i in layer_info]\n",
    "    layer_dims.insert(0,X.shape[0])\n",
    "    \n",
    "    return layer_info , layer_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da122a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_info = None\n",
    "layer_dims = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5265d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, no_of_epoch, learning_rate=0.075):\n",
    "    global layer_info , layer_dims\n",
    "    layer_info , layer_dims = layer_initialization()\n",
    "    parameters = initialize_params(layer_dims)\n",
    "    \n",
    "    for i in range(no_of_epoch):\n",
    "        AL, cache = forward(X,parameters) \n",
    "        cost = compute_cost(AL,Y)\n",
    "        grads = backward(AL,Y,X,cache,parameters)\n",
    "        parameters = update_params(parameters,grads,learning_rate)\n",
    "\n",
    "        if i%100 == 0:\n",
    "            Y_predict_test = predict(test_x_set,parameters)\n",
    "            Y_predict_train = predict(train_x_set,parameters)\n",
    "            print(\"Cost after \",str(i),\" iteration : \",cost,end='')\n",
    "            print(\"\\ttrain accuracy: {} %\".format(100 - np.mean(np.abs(Y_predict_train - train_y_set)) * 100),end='')\n",
    "            print(\"\\ttest accuracy: {} %\".format(100 - np.mean(np.abs(Y_predict_test - test_y_set)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931f5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(X, Y, no_of_epoch=2000, learning_rate=0.075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5c71e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0a883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06733426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f045394",
   "metadata": {},
   "source": [
    "## 2- L2 Regularization\n",
    "\n",
    "The standard way to avoid overfitting is called **L2 regularization**. It consists of appropriately modifying your cost function, from:\n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} \\tag{1}$$\n",
    "To:\n",
    "$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}$$\n",
    "\n",
    "Let's modify your cost and observe the consequences.\n",
    "\n",
    "Implement `compute_cost_with_regularization()` which computes the cost given by formula (2). To calculate $\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2}$  , use :\n",
    "```python\n",
    "np.sum(np.square(Wl))\n",
    "```\n",
    "Note that you have to do this for $W^{[1]}$, $W^{[2]}$ and $W^{[3]}$, then sum the three terms and multiply by $ \\frac{1}{m} \\frac{\\lambda}{2} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e7ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(AL, Y, parameters, lambd):\n",
    "    \"\"\"\n",
    "    Implement the cost function with L2 regularization. See formula (2) above.\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- post-activation, output of forward propagation, of shape (output size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    parameters -- python dictionary containing parameters of the model\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the regularized loss function (formula (2))\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    L = int(len(parameters)/2)\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(1,L+1):\n",
    "        total += np.sum(np.square(parameters['W'+str(i)]))\n",
    "    regularization_cost = total * lambd / (2*m)\n",
    "    \n",
    "    cost = - (np.dot(Y, np.log(AL).T) + np.dot((1 - Y), np.log(1 - AL).T)) / m\n",
    "    cost = np.squeeze(cost) \n",
    "    \n",
    "    \n",
    "    return cost + regularization_cost\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe8944",
   "metadata": {},
   "source": [
    "because you changed the cost, you have to change backward propagation as well! All the gradients have to be computed with respect to this new cost. \n",
    "\n",
    "Implement the changes needed in backward propagation to take into account regularization. The changes only concern dW1, dW2,.... For each, you have to add the regularization term's gradient ($\\frac{d}{dW} ( \\frac{1}{2}\\frac{\\lambda}{m}  W^2) = \\frac{\\lambda}{m} W$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b319a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_with_regularization(AL,Y,X,caches,parameters,lambd):\n",
    "    grads = {}\n",
    "    L = int(len(caches)/2)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) \n",
    "    \n",
    "    dA = -(Y/AL - (1-Y)/(1-AL))\n",
    "    \n",
    "    for i in range(L,1,-1):\n",
    "        \n",
    "        if layer_info[i-1][1] == 'relu':\n",
    "            dZ = np.array(dA, copy=True)\n",
    "            dZ[caches['Z'+str(i)] <= 0] = 0\n",
    "        elif layer_info[i-1][1] == 'sigmoid':\n",
    "            s = 1 / (1 + np.exp(-caches['Z'+str(i)]))\n",
    "            dZ = dA * s * (1-s)\n",
    "        elif layer_info[i-1][1] == 'tanh':\n",
    "            dZ = np.dot(parameters['W'+str(i+1)].T,dZ) * (1 - np.power(caches['A'+str(i)],2))\n",
    "            \n",
    "        grads['dW'+str(i)] = np.dot(dZ,caches['A'+str(i-1)].T) / m + (lambd/m * parameters['W'+str(i)])\n",
    "        grads['dB'+str(i)] = np.sum(dZ) / m\n",
    "        \n",
    "        dA = np.dot(parameters['W'+str(i)].T,dZ)\n",
    "    \n",
    "    #  for first layer\n",
    "    if layer_info[0][1] == 'relu':\n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[caches['Z'+str(1)] <= 0] = 0\n",
    "    elif layer_info[0][1] == 'sigmoid':\n",
    "        s = 1 / (1 + np.exp(-caches['Z'+str(1)]))\n",
    "        dZ = dA * s * (1-s)\n",
    "    elif layer_info[0][1] == 'tanh':\n",
    "        dZ = np.dot(parameters['W'+str(2)].T,dZ) * (1 - np.power(caches['A'+str(1)],2))\n",
    "    \n",
    "    grads['dW1'] = np.dot(dZ,X.T) / m + (lambd/m * parameters['W'+str(1)])\n",
    "    grads['dB1'] = np.sum(dZ) / m\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca32e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_regularization_model(X, Y, no_of_epoch, learning_rate=0.075,lambd=0.5):\n",
    "    global layer_info , layer_dims\n",
    "    layer_info , layer_dims = layer_initialization()\n",
    "    parameters = initialize_params(layer_dims)\n",
    "    \n",
    "    for i in range(no_of_epoch):\n",
    "        AL, cache = forward(X,parameters) \n",
    "        cost = compute_cost_with_regularization(AL,Y,parameters,lambd)\n",
    "        grads = backward_with_regularization(AL,Y,X,cache,parameters,lambd)\n",
    "        parameters = update_params(parameters,grads,learning_rate)\n",
    "\n",
    "        if i%100 == 0:\n",
    "            Y_predict_test = predict(test_x_set,parameters)\n",
    "            Y_predict_train = predict(train_x_set,parameters)\n",
    "            print(\"Cost after \",str(i),\" iteration : \",cost,end='')\n",
    "            print(\"\\ttrain accuracy: {} %\".format(100 - np.mean(np.abs(Y_predict_train - train_y_set)) * 100),end='')\n",
    "            print(\"\\ttest accuracy: {} %\".format(100 - np.mean(np.abs(Y_predict_test - test_y_set)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a1700",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_regularization_model(X, Y, no_of_epoch=2000, learning_rate=0.075,lambd=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feebe690",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- The value of $\\lambda$ is a hyperparameter that you can tune using a dev set.\n",
    "- L2 regularization makes your decision boundary smoother. If $\\lambda$ is too large, it is also possible to \"oversmooth\", resulting in a model with high bias.\n",
    "\n",
    "**What is L2-regularization actually doing?**:\n",
    "\n",
    "L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde82df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360cb6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ad6d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40affa3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e4a9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f64e866",
   "metadata": {},
   "source": [
    "## 3 - Dropout\n",
    "\n",
    "**dropout** is a widely used regularization technique that is specific to deep learning. \n",
    "**It randomly shuts down some neurons in each iteration.** Watch these two videos to see what this means!\n",
    "\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/dropout1_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "<br>\n",
    "<caption><center> <u> Figure 2 </u>: Drop-out on the second hidden layer. <br> At each iteration, you shut down (= set to zero) each neuron of a layer with probability $1 - keep\\_prob$ or keep it with probability $keep\\_prob$ (50% here). The dropped neurons don't contribute to the training in both the forward and backward propagations of the iteration. </center></caption>\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/dropout2_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "<caption><center> <u> Figure 3 </u>: Drop-out on the first and third hidden layers. <br> $1^{st}$ layer: we shut down on average 40% of the neurons.  $3^{rd}$ layer: we shut down on average 20% of the neurons. </center></caption>\n",
    "\n",
    "\n",
    "When you shut some neurons down, you actually modify your model. The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time. \n",
    "\n",
    "### Forward propagation with dropout\n",
    "\n",
    "Implement the forward propagation with dropout. You are using a L layer neural network, and will add dropout to the layers with relu activation. We will not apply dropout to the input layer or output layer. \n",
    "\n",
    "**Instructions**:\n",
    "You would like to shut down some neurons in the layers with relu activation. To do that, you are going to carry out 4 Steps:\n",
    "\n",
    "1. creating a variable $d^{[1]}$ with the same shape as $a^{[1]}$ using `np.random.rand()` to randomly get numbers between 0 and 1. Here, you will use a vectorized implementation, so create a random matrix $D^{[1]} = [d^{[1](1)} d^{[1](2)} ... d^{[1](m)}] $ of the same dimension as $A^{[1]}$.\n",
    "\n",
    "2. Set each entry of $D^{[1]}$ to be 0 with probability (`1-keep_prob`) or 1 with probability (`keep_prob`), by thresholding values in $D^{[1]}$ appropriately. Hint: to set all the entries of a matrix X to 0 (if entry is less than 0.5) or 1 (if entry is more than 0.5) you would do: `X = (X < 0.5)`. Note that 0 and 1 are respectively equivalent to False and True.\n",
    "\n",
    "3. Set $A^{[1]}$ to $A^{[1]} * D^{[1]}$. (You are shutting down some neurons). You can think of $D^{[1]}$ as a mask, so that when it is multiplied with another matrix, it shuts down some of the values.\n",
    "\n",
    "4. Divide $A^{[1]}$ by `keep_prob`. By doing this you are assuring that the result of the cost will still have the same expected value as without drop-out. (This technique is also called inverted dropout.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236393ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n",
    "    A = X\n",
    "    cache = {}\n",
    "    drop =  {}\n",
    "    L = int(len(parameters)/2)\n",
    "    \n",
    "    for i in range(1,L+1):\n",
    "        A_prev = A\n",
    "        \n",
    "        Z = np.dot(parameters['W'+str(i)],A_prev) + parameters['B'+str(i)]\n",
    "        \n",
    "        if layer_info[i-1][1] == 'relu':\n",
    "            A = np.maximum(0,Z)\n",
    "            D = np.random.rand(Z.shape[0],Z.shape[1])     # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n",
    "            D = (D < keep_prob) * 1     # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n",
    "            drop['D'+str(i)] = D\n",
    "            A = A * D                   # Step 3: shut down some neurons of A1\n",
    "            A = A / keep_prob           # Step 4: scale the value of neurons that haven't been shut down\n",
    "        elif layer_info[i-1][1] == 'sigmoid':\n",
    "            A = 1 / ( 1 + np.exp(-Z) )\n",
    "        elif layer_info[i-1][1] == 'tanh':\n",
    "            A = np.tanh(Z)\n",
    "        cache['Z'+str(i)] = Z\n",
    "        cache['A'+str(i)] = A\n",
    "\n",
    "    return  A, cache, drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e3d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "357b613b",
   "metadata": {},
   "source": [
    "### 3.2 - Backward propagation with dropout\n",
    "\n",
    "Implement the backward propagation with dropout. As before, you are training a L layer network. Add dropout to the layers with relu activation, using the masks $D^{[1]}$ and $D^{[2]}$ stored in the cache. \n",
    "\n",
    "**Instruction**:\n",
    "Backpropagation with dropout is actually quite easy. You will have to carry out 2 Steps:\n",
    "1. You had previously shut down some neurons during forward propagation, by applying a mask $D^{[1]}$ to `A1`. In backpropagation, you will have to shut down the same neurons, by reapplying the same mask $D^{[1]}$ to `dA1`. \n",
    "2. During forward propagation, you had divided `A1` by `keep_prob`. In backpropagation, you'll therefore have to divide `dA1` by `keep_prob` again (the calculus interpretation is that if $A^{[1]}$ is scaled by `keep_prob`, then its derivative $dA^{[1]}$ is also scaled by the same `keep_prob`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b2c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_dropout(AL,Y,X, caches,parameters,drop, keep_prob=0.5):\n",
    "    grads = {}\n",
    "    L = int(len(caches)/2)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) \n",
    "    \n",
    "    dA = -(Y/AL - (1-Y)/(1-AL))\n",
    "    \n",
    "    for i in range(L,1,-1):\n",
    "        \n",
    "        if layer_info[i-1][1] == 'relu':\n",
    "            dZ = np.array(dA, copy=True)\n",
    "            dZ[caches['Z'+str(i)] <= 0] = 0\n",
    "            \n",
    "            grads['dW'+str(i)] = np.dot(dZ,caches['A'+str(i-1)].T) / m\n",
    "            grads['dB'+str(i)] = np.sum(dZ) / m\n",
    "        \n",
    "            dA = np.dot(parameters['W'+str(i)].T,dZ)\n",
    "            dA = dA * drop['D'+str(i-1)]     # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n",
    "            dA = dA / keep_prob              # Step 2: Scale the value of neurons that haven't been shut down\n",
    "        \n",
    "        elif layer_info[i-1][1] == 'sigmoid':\n",
    "            s = 1 / (1 + np.exp(-caches['Z'+str(i)]))\n",
    "            dZ = dA * s * (1-s)\n",
    "            \n",
    "            grads['dW'+str(i)] = np.dot(dZ,caches['A'+str(i-1)].T) / m\n",
    "            grads['dB'+str(i)] = np.sum(dZ) / m\n",
    "        \n",
    "            dA = np.dot(parameters['W'+str(i)].T,dZ)\n",
    "            dA = dA * drop['D'+str(i-1)]\n",
    "            dA = dA / keep_prob\n",
    "            \n",
    "        elif layer_info[i-1][1] == 'tanh':\n",
    "            dZ = np.dot(parameters['W'+str(i+1)].T,dZ) * (1 - np.power(caches['A'+str(i)],2))\n",
    "            \n",
    "            grads['dW'+str(i)] = np.dot(dZ,caches['A'+str(i-1)].T) / m\n",
    "            grads['dB'+str(i)] = np.sum(dZ) / m\n",
    "        \n",
    "            dA = np.dot(parameters['W'+str(i)].T,dZ)\n",
    "    \n",
    "    #  for first layer\n",
    "    if layer_info[0][1] == 'relu': \n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[caches['Z'+str(1)] <= 0] = 0\n",
    "    elif layer_info[0][1] == 'sigmoid':\n",
    "        s = 1 / (1 + np.exp(-caches['Z'+str(1)]))\n",
    "        dZ = dA * s * (1-s)\n",
    "    elif layer_info[0][1] == 'tanh':\n",
    "        dZ = np.dot(parameters['W'+str(2)].T,dZ) * (1 - np.power(caches['A'+str(1)],2))\n",
    "    \n",
    "    grads['dW1'] = np.dot(dZ,X.T) / m\n",
    "    grads['dB1'] = np.sum(dZ) / m\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4741044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_model(X, Y, no_of_epoch, learning_rate=0.075,keep_prob=0.5):\n",
    "    global layer_info , layer_dims\n",
    "    layer_info , layer_dims = layer_initialization()\n",
    "    parameters = initialize_params(layer_dims)\n",
    "    \n",
    "    for i in range(no_of_epoch):\n",
    "        AL, cache, drop = forward_propagation_with_dropout(X,parameters,keep_prob) \n",
    "        cost = compute_cost(AL,Y)\n",
    "        grads = backward_propagation_with_dropout(AL,Y,X,cache,parameters,drop, keep_prob)\n",
    "        parameters = update_params(parameters,grads,learning_rate)\n",
    "\n",
    "        if i%100 == 0:\n",
    "            Y_predict_test = predict(test_x_set,parameters)\n",
    "            Y_predict_train = predict(train_x_set,parameters)\n",
    "            print(\"Cost after \",str(i),\" iteration : \",cost,end='')\n",
    "            print(\"\\ttrain accuracy: {} %\".format(100 - np.mean(np.abs(Y_predict_train - train_y_set)) * 100),end='')\n",
    "            print(\"\\ttest accuracy: {} %\".format(100 - np.mean(np.abs(Y_predict_test - test_y_set)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d1f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_model(X, Y, no_of_epoch=2000, learning_rate=0.075,keep_prob=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8008d9d9",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "- A **common mistake** when using dropout is to use it both in training and testing. You should use dropout (randomly eliminate nodes) only in training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc688bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e81679e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
